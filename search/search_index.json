{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ML Notes","text":"<p>Welcome to my machine learning notes for personal reference and sharing.</p>"},{"location":"#quick-start","title":"Quick start","text":"<ul> <li>Install and serve locally:</li> </ul> <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\nmkdocs serve\n</code></pre> <ul> <li>To publish: create the GitHub repo <code>pushkarnagar.github.io</code> and push (commands in README).</li> </ul>"},{"location":"#structure-suggestions","title":"Structure suggestions","text":"<ul> <li>Organize by topic folders under <code>docs/</code> (e.g., <code>00-basics</code>, <code>01-deep-learning</code>).</li> <li>Add notebooks converted to Markdown in <code>docs/notes/</code> or use <code>mkdocs-jupyter</code> plugin to render them directly.</li> </ul>"},{"location":"00-basics/linear_regression/","title":"Linear Regression","text":"<pre><code># Linear Regression\n\nConcise study notes on ordinary least squares linear regression.\n\n## At a glance\n\n- Model: linear mapping from features to target.\n- Use closed-form solution for small datasets; use gradient methods or regularized solvers otherwise.\n\n## Model\n\nPredict with weights $w$ and bias $b$:\n\n$y = w^{\\top} x + b$\n\nThe normal-equation (closed-form) solution is\n\n$$\nw = (X^{\\top} X)^{-1} X^{\\top} y\n$$\n\nwhere $X$ is the n\u00d7d design matrix (rows = examples).\n\n## Loss\n\n- Mean squared error (MSE):\n\n$$\n\\mathrm{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n$$\n\n## Practical tips\n\n- Regularization: Ridge adds $\\\\lambda \\\\|w\\\\|_2^2$, Lasso adds $\\\\lambda \\\\|w\\\\|_1$.\n- Center and scale features for gradient-based solvers.\n- Check condition number of $X^{\\top}X$; if ill-conditioned prefer regularization or SVD.\n\n## Minimal Python example\n\n```python\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nX = np.array([[1], [2], [3], [4]])\ny = np.array([3, 5, 7, 9])  # approx y = 2x + 1\n\nmodel = LinearRegression().fit(X, y)\nprint('coef=', model.coef_, 'intercept=', model.intercept_)\n```\n\n## References\n\n- Ordinary Least Squares, linear algebra textbooks.\n- scikit-learn `LinearRegression` and `Ridge`, `Lasso` implementations.\n\n## Assumptions\n\nCommon assumptions for ordinary least squares (OLS) linear regression:\n\n- **Linearity:** the conditional expectation is linear: $E[y\\\\mid x]=w^{\\\\top}x+b$.\n- **Independence:** observations are independent (no autocorrelation).\n- **Homoscedasticity:** error variance is constant: $\\\\mathrm{Var}(\\\\varepsilon_i)=\\\\sigma^2$.\n- **Zero mean errors / exogeneity:** $E[\\\\varepsilon\\\\mid x]=0$ (no omitted-variable bias).\n- **No perfect multicollinearity:** columns of $X$ are linearly independent so $X^{\\\\top}X$ is invertible.\n- **(Optional for inference) Normality of errors:** $\\\\varepsilon\\\\sim N(0,\\\\sigma^2)$ simplifies confidence intervals and tests.\n\nViolations affect estimation or inference: heteroskedasticity breaks OLS efficiency (use robust SEs), autocorrelation requires time-series methods, and endogeneity requires instrumental variables.\n\n## Special cases &amp; common remedies\n\n- **Positive-only targets ($y&gt;0$):** if the response is strictly positive consider a log transform ($\\\\log y$) and model linearity in log-space, or use a GLM (Gamma family with log link) or Poisson/Quasi-Poisson for counts.\n- **Count data / many zeros:** use Poisson, negative binomial, or zero-inflated models instead of OLS.\n- **Bounded outcomes (e.g., probabilities):** use logistic regression or Beta regression rather than OLS.\n- **Heteroskedasticity:** use weighted least squares (WLS) or heteroskedasticity-robust standard errors (HC0\u2013HC3).\n- **Multicollinearity / high-dimensional data (p &gt; n):** use Ridge, Lasso, or dimensionality reduction (PCA); drop or combine correlated features.\n- **Outliers / heavy tails:** use robust regression (Huber, RANSAC) or transform the outcome.\n- **Singular/ill-conditioned $X^{\\\\top}X$:** use regularization or compute a pseudo-inverse via SVD.\n\nKeep explanations short and add examples or Jupyter notebooks when you want runnable demonstrations.\n</code></pre>"},{"location":"00-basics/linear_regression/#linear-regression","title":"Linear Regression","text":"<p>Concise study notes on ordinary least squares linear regression.</p>"},{"location":"00-basics/linear_regression/#at-a-glance","title":"At a glance","text":"<ul> <li>Model: linear mapping from features to target.</li> <li>Use closed-form solution for small datasets; use gradient methods or regularized solvers otherwise.</li> </ul>"},{"location":"00-basics/linear_regression/#model","title":"Model","text":"<p>Predict with weights $w$ and bias $b$:</p> <p>$y = w^{\\top} x + b$</p> <p>The normal-equation (closed-form) solution is</p> <p>$$ w = (X^{\\top} X)^{-1} X^{\\top} y $$</p> <p>where $X$ is the n\u00d7d design matrix (rows = examples).</p>"},{"location":"00-basics/linear_regression/#loss","title":"Loss","text":"<ul> <li>Mean squared error (MSE):</li> </ul> <p>$$ \\mathrm{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 $$</p>"},{"location":"00-basics/linear_regression/#practical-tips","title":"Practical tips","text":"<ul> <li>Regularization: Ridge adds $\\lambda |w|_2^2$, Lasso adds $\\lambda |w|_1$.</li> <li>Center and scale features for gradient-based solvers.</li> <li>Check condition number of $X^{\\top}X$; if ill-conditioned prefer regularization or SVD.</li> </ul>"},{"location":"00-basics/linear_regression/#minimal-python-example","title":"Minimal Python example","text":"<pre><code>import numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nX = np.array([[1], [2], [3], [4]])\ny = np.array([3, 5, 7, 9])  # approx y = 2x + 1\n\nmodel = LinearRegression().fit(X, y)\nprint('coef=', model.coef_, 'intercept=', model.intercept_)\n</code></pre>"},{"location":"00-basics/linear_regression/#references","title":"References","text":"<ul> <li>Ordinary Least Squares, linear algebra textbooks.</li> <li>scikit-learn <code>LinearRegression</code> and <code>Ridge</code>, <code>Lasso</code> implementations.</li> </ul>"}]}